# Proxy settings
# HTTP_PROXY=http://proxy.example.com:8080
# HTTPS_PROXY=http://proxy.example.com:8080
# NO_PROXY=localhost,127.0.0.1

# Container settings
CONTAINER_USER=devai
IMAGE_NAME=devai-lab
PORT=8888
CONTAINER_RUNTIME=podman
HOST_WORK_DIR=.
HOST_HOME_DIR=

# Ollama settings (for local model inference)
# Run Ollama on host: ollama serve
# Default connects to host machine's Ollama instance
OLLAMA_HOST=http://host.containers.internal:11434
